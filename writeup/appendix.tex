\section{Deferred Explanations}
\label{sec:deferred-proofs}

\subsection{How a dealer is ``bound'' to a committed value}
\label{sec:dealer-bound-to-commited-value}

In section \ref{sec:building-blocks-for-the-protocol} we said that the dealer is bound to his shared value once he committed to it. Of course, there are several ways to implement this, but we will briefly describe one from \cite{lecture-notes-goldwasser-bellare}.

We use a one-way collision-free hash function $H$ (a one-way function is a function that can be easily evaluated, but whose inverse is very complicated or impossible to compute). Then, if we want to commit to some value $x$, we can send as a ``promise'' the value $y=H(x)$. Note that -- informally stated -- nobody is able (at least not in feasible time) to find another value $x\neq y$ with $H(x)=H(x')=y$.

In this way, we are bound to the value $x$, since we already published $y=H(x)$, but cannot find another value that yields $y$ when we apply $H$ to it. Thus, if we later publish our secret $x$, the other players simply need to check whether they obtain $y$ when they apply $H$ to it. This way, we cannot change our mind without being caught.

\subsection{The basic idea of secret sharing}
\label{sec:appendix-basic-idea-secret-sharing}

Consider the following basic idea for secret sharing. Assume you have some bit $s$ that you want to share among three players such that no player con reconstruct the value on her own (but they should be able to reconstruct it altogether). 

Then, you could toss two coins yielding random bits $s_1$ and $s_2$. Moreover, you construct a third bit $s_3=s_1\oplus s_2\oplus s$. Note that then $s=s_1\oplus s_2 \oplus s_3$. Then you give $s_1$ to player 1, $s_2$ to player 2 and so forth. Afterwards the three players can -- collaboratively -- reconstruct the value $s$, but none of them can do so on her own.

This is -- of course -- a very simple scheme, that only works if all players are honest and if \emph{all} players join the collaborative computation.

However, there are more sophisticated techniques (e.g. relying on polynomials over finite fields like $\mathbb Z_p$ for some prime $p$) that allow a certain amount of dishonest players and still work. 

For more information on that topic, please refer to \cite{shamir_secret_sharing}.

\subsection{Collaborative coin flipping -- basic idea}
\label{sec:appendix-coin-flipping}

Taking the XOR (evaluating securely and secretly) of all these bits returns the random bit generated by all the participants in common. This computation (XOR on any number of bits) can be done in a constant number of rounds and using only polynomial amount of communication. This can be seen, for example, by a deeper investigation of the arguments in \cite{beaver-verifiable-secret-sharing} (according to \cite{Beaver1990}).

From now on, we use the symbol $\oplus$ to denote the XOR-computation. Even if XOR is usually defined on single bits only, we apply this function to bit strings bit-wise (e.g. $1100\oplus1001=0101$).

\subsection{Some words on Pseudorandom Generators}
\label{sec:appendix-pseudorandom-generators}

It is a widely known fact that pseudorandom generators exist if and only if so-called \emph{one-way} functions exist. These are functions that are easy to compute, but hard to invert. For more information on the relation between one-way functions and pseudorandom generators, please refer to \cite{lecture-notes-goldwasser-bellare,yao-theory-application-trapdoor-functions,pseudorandom-generators-blum}.

Even now, whether such one-way functions exist is an open question. Thus, it is -- as mentioned in section \ref{sec:building-blocks-for-the-protocol} -- not proven that pseudorandom generators exist at all.

One remark that has to be made in this context is the following: The existence of one-way functions (or -- equivalently -- of pseudorandom generators) directly implies $\mathbf{P}\neq\mathbf{NP}$ ($\mathbf{P}$ and $\mathbf{NP}$ being the famous complexity classes, of course). This is simply, because the inverse of such a one-way function would be ``hard to compute, but easy to verify'', which is -- informally -- what $\mathbf{NP}$ is about.

Thus, a proof for the existence of pseudorandom generators is probably -- to put it mildly -- none of the simple-to-read ones.

\subsection{Size of two-fan-in-circuits}
\label{sec:size-two-fan-circuits}

We can reduce any circuit to a circuit consisting only of gates having two input wires, increasing its size only by a polynomial factor.

To prove this, consider an AND-gate having $n>2$ wires. Then, we can simulate this gate using only AND-gates with two input wires. 

We do this in a manner similar to a complete binary tree (see Figure \ref{fig:complete-binary-tree}). 

\begin{figure}[t]
  \centering
  \subfigure[A single AND-gate with eight inputs.]{
    \begin{tikzpicture}[xscale=3]
      \draw(0,0) rectangle (1.6,0.4);
      \draw[->, semithick] (0.1,-.4) -- (0.1,0);
      \draw[->, semithick] (0.3,-.4) -- (0.3,0);
      \draw[->, semithick] (0.5,-.4) -- (0.5,0);
      \draw[->, semithick] (0.7,-.4) -- (0.7,0);
      \draw[->, semithick] (0.9,-.4) -- (0.9,0);
      \draw[->, semithick] (1.1,-.4) -- (1.1,0);
      \draw[->, semithick] (1.3,-.4) -- (1.3,0);
      \draw[->, semithick] (1.5,-.4) -- (1.5,0);
      \draw[->, semithick] (.8,.4) -- (.8,1.4);
    \end{tikzpicture}
  }
  \hspace{1cm}
  \subfigure[Several 2-input AND gates simulate one 8-input AND gate.]{
  \centering
  \begin{tikzpicture}
    [level distance=5mm, 
    level 1/.style={sibling distance=32mm},
    level 2/.style={sibling distance=16mm},
    level 3/.style={sibling distance=8mm},
    level 4/.style={sibling distance=4mm},
    semithick,
    every node/.style={draw=black,<-},
    %edge from parent fork down,
    ]
    \node[semithick] (root) {}
    child {node{} 
      child {node{}
      }
      child {node{}
      }
    }
    child {node{} 
      child {node{}
      }
      child {node{}}
    };
    % \draw[<-] (root-1-1-1) -- +(0,-.5);
    % \draw[<-] (root-1-1-2) -- +(0,-.5);
    % \draw[<-] (root-1-2-1) -- +(0,-.5);
    % \draw[<-] (root-1-2-2) -- +(0,-.5);
    % \draw[<-] (root-2-1-1) -- +(0,-.5);
    % \draw[<-] (root-2-1-2) -- +(0,-.5);
    \draw[<-] (root-1-1)   -- +(.5,-.5) -- +(.5,-1);
    \draw[<-] (root-1-1)   -- +(-.5,-.5) -- +(-.5,-1);
    \draw[<-] (root-1-2)   -- +(.5,-.5) -- +(.5,-1);
    \draw[<-] (root-1-2)   -- +(-.5,-.5) -- +(-.5,-1);
    \draw[<-] (root-2-1)   -- +(.5,-.5) -- +(.5,-1);
    \draw[<-] (root-2-1)   -- +(-.5,-.5) -- +(-.5,-1);
    \draw[<-] (root-2-2)   -- +(.5,-.5) -- +(.5,-1);
    \draw[<-] (root-2-2)   -- +(-.5,-.5) -- +(-.5,-1);
    \draw[->] (root) -- + (0,.4);
  \end{tikzpicture}
  }
  \caption{Simulating an $n$-input AND-gate by several 2-input AND-gates}
  \label{fig:complete-binary-tree}
\end{figure}

The bottom level consists of at most $\frac{n}{2}$ AND-gates. The second-to-bottom level consists of at most $\frac{n}{4}$ gates. The next level has at most $\frac{n}{8}$ gates, and so forth. Thus, the total number of gates used is at most $\sum_{i=1}^{\log n} \frac{n}{2^i}\leq n$.

Thus, for one AND-gate that has $n$ inputs, we can construct a sub-circuit consisting of at most $n$ gates such that this sub-circuit resembles the computation of the original AND-gate. The size increased by a polynomial factor (in the number of inputs of the original gate). 

For other types of gates, we can use similar constructions.

Since any gate can have at most as many inputs as there are gates in the whole circuit, the size of the total circuit increases by a polynomial factor in the size of the circuit.

Note that this idea does not only work for AND gates, but also for other gates. In particular, it is working for splitter gates (as introduced in section \ref{sec:introducing-splitters}).

\subsection{Why the use of Pseudorandom Generators?}
\label{sec:appendix-why-pseudorandom-generators}

Let us for a moment consider equation (\ref{eq:gate-label-definition-with-output}) and propose the following question: The output of $G_0$ resp. $G_1$ on some $k$-bit string is a binary string of length $nk+1$. So, one might argue that it would be simpler to work directly on the signals $\sigma=\sigma_1\dots\sigma_n a$ and $\tau=\tau_1\dots\tau_n b$ instead of first splitting up the signals into its components and then considering $G_b(\sigma_i)$ and $G_a(\tau_j)$, respectively.

While this simplification would be useful if we were just interested in correctness, we would lose privacy, if this was done. That is, a player possibly can deduce the semantics of some signals -- which is something we strictly want to prevent.

To see this, consider a simple circuit computing the function $(x\wedge y_1) \wedge y_2$. This circuit is depicted in figure \ref{fig:circuit-used-for-pseudorandom-generator}. Assume that this circuit does not use pseudorandom generators, but instead operates directly on the signals. We will now inspect how the player supplying $x$ can deduce the plain-text of $y_2$, even if he supplies $x=0$.

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[semithick]
    \draw[] (0,0) rectangle (1,.5);
    \draw (0.5,1) rectangle (1.5,1.5);
    \draw[<-] (0.25, 0) -- +(0,-.5) node[below]{$\alpha$};
    \draw[<-] (0.75, 0) -- +(0,-.5) node[below]{${\beta}_1$};
    \draw[<-] (1.25, 1) -- +(0,-1.5) node[below]{${\beta}_2$};
    \draw[->] (.5,.5) -- node[left,yshift=.15cm]{$\gamma$} +(0,.25) -| (0.75, 1);
    \draw[->] (1,1.5) -- +(0,.5) node [right]{$\delta$};
    \node[] at (.5,.25) {$M_1$};
    \node at (1,1.25) {$M_2$};
  \end{tikzpicture}
  \caption{A circuit computing $(x\wedge y_1)\wedge y_2$. Wire $\alpha$ carries the signal for $x$, while wires $\beta_1$ and $\beta_2$ carry the signals for signals $y_1$ and $y_2$, respectively. All gates are AND-gates.}
  \label{fig:circuit-used-for-pseudorandom-generator}
\end{figure}

Assume that the wire for $x$ is called $\alpha$, the wires for $y_1$ and $y_2$ are $\beta_1$ and $\beta_2$, respectively. First, we take the gate $M_1$ (input wires $\alpha$ and $\beta_1$) into account. The respective output wire (holding the garbled signal for $x\wedge y_1$) is denoted by $\gamma$.

Since we obtain the garbled circuit and the garbled input signals, we already know the (garbled) signals along $\alpha$ and $\beta_1$. We assume them without loss of generality to be of parity $0$.

So, we know the signal $s^\alpha_0$ and that this signal represents plain-text $\mathbf{0}$ (i.e. we know $s^\alpha_0$ and $s^\alpha_0 \leftrightarrow \mathbf{0}$). Moreover, we know $s_0^{\beta_1}$, but we do not know what $s_0^{\beta_1}$ stands for (we do not know the plain-text). We then -- of course -- can compute the outgoing signal along wire $\gamma$ (using the gate labels). Without loss of generality, we assume that the output signal has parity $0$ (that is, we have signal $s_0^\gamma$ along wire $\gamma$). We know that $s_0^\gamma \leftrightarrow \mathbf{0}$ since we supplied the signal for $x=0$ and, thus, $(x\wedge y_1)=(0\wedge y_1) = 0$.

Using only these things, and since we know \emph{all} gate labels, we can now compute some other values: We know that if we supply the signal $s_1^{\beta_1}$, the outcome along wire $\gamma$ must still represent $\mathbf{0}$, since the gate represents an AND and $s_0^\alpha\leftrightarrow \mathbf{0}$. Thus, we can say that $s_0^\gamma=A_{01}\oplus s_0^\alpha \oplus s_1^{\beta_1}$. We can solve this equation for $s_1^{\beta_1}$ (we know $A_{01}, s_0^\alpha, s_0^\gamma$). That is, by now we have both signals for wire $\beta_1$.

We now introduce the following notation: $\sigma_{xy}^\omega$ denotes the signal that would be obtained along wire $\omega$ if we used signals of parities $x$ and $y$ as inputs to the corresponding gate.

Next, we consider two equations:

\begin{eqnarray}
\label{eq:gate-trick-1}
\sigma_{10}^\gamma &=& A_{10} \oplus s_1^\alpha \oplus s_0^{\beta_1} \\
\label{eq:gate-trick-2}
\sigma_{11}^\gamma &=& A_{11} \oplus s_1^\alpha \oplus s_1^{\beta_1}
\end{eqnarray}

As said above, $\sigma_{10}^\gamma$ denotes the signal we obtain if we supply $s_1^\alpha$ and $s_0^\beta$ along the input wires to the gate. So these two equations basically tell us what the output would be if we supplied particular input signals. Note that \emph{exactly one} of the following holds: 

\begin{equation*}
  (s_0^\gamma = \sigma_{01}^\gamma \, \wedge \, s_1^\gamma = \sigma_{11}^\gamma) \quad \text{ or } \quad (s_1^\gamma = \sigma_{01}^\gamma \, \wedge \, s_0^\gamma = \sigma_{11}^\gamma).
\end{equation*}

We solve equations (\ref{eq:gate-trick-1}) and (\ref{eq:gate-trick-2}) for $s_1^\alpha$, and deduce that
\begin{equation*}
  \sigma_{10}^\gamma\oplus\sigma_{11}^\gamma = A_{10}\oplus A_{11} \oplus s_0^{\beta_1} \oplus s_1^{\beta_1}.
\end{equation*}

Thus, the left-hand-side $\sigma_{10}^\gamma\oplus\sigma_{11}^\gamma$ denotes exactly the ``bit-difference'' of $s_0^\gamma$ and $s_1^\gamma$. This means, we can deduce the value $s_1^\gamma$ (simply by computing $s_1^\gamma = s_0^\gamma \oplus (A_{10}\oplus A_{11} \oplus s_0^{\beta_1} \oplus s_1^{\beta_1})$) and we know that $s_1^\gamma\leftrightarrow \mathbf{1}$.

Now we go onto the second gate $M_2$ and recognize that we know \emph{all} signals for wire $\gamma$ with the associated plain-text values and the signal for wire $\beta_2$ representing the plain-text $y_2$. Without loss of generality, we assume that the parity of the signal for $y_2$ is $0$ (but we do not know what this parity-0-signal stands for).

We know that if we supply all our original garbled signals (in particular the signal for $x=0$), the output of $M_2$ must represent plain-text 0, since $(0\wedge y_1)\wedge y_2=0$. We assume without loss of generality that $s_0^\delta \leftrightarrow \mathbf{0}$ (i.e. that the original garbled output signal representing plain-text $\mathbf{0}$ has parity 0).

Analogously to the gate $M_1$, we deduce $s_1^{\beta_2}$ from the given values. That is, we now know for gate $M_2$ the following:

\begin{itemize}
\item Left input signals and associated semantics (i.e. we know $s_0^\gamma$ and $s_1^\gamma$ and $s_0^\gamma\leftrightarrow\mathbf{0}, s_1^\gamma\leftrightarrow\mathbf{1}$)
\item Right input signals, but not yet associated semantics (i.e. we know $s_0^{\beta_2}, s_1^{\beta_2}$)
\item Output signal for plain-text 0 (i.e. we know $s_0^\delta$ and $s_0^\delta\leftrightarrow \mathbf{0}$)
\end{itemize}

Using all these, we can simply ``test'' gate $M_2$ by supplying several signals and looking at the output signal: We supply all \emph{original} garbled signals and know that the output must represent 0. Then, we supply $s_1^\gamma$ (representing plain-text 1) and check which signal for wire $\beta_2$ changes the output.

To be more precise, we compute $\sigma_{10}^\delta$ and $\sigma_{11}^\delta$ as follows:

\begin{eqnarray*}
  \sigma_{10}^\delta &=& A^{M_2}_{10} \oplus s_1^\gamma \oplus s_0^{\beta_2} \\
  \sigma_{11}^\delta &=& A^{M_2}_{11} \oplus s_1^\gamma \oplus s_1^{\beta_2}
\end{eqnarray*}

If $\sigma_{10}^\delta=s_0^\delta$, then $s_0^{\beta_2}$ represents plain-text $\mathbf{0}$. If $\sigma_{10}^\delta=s_1^\delta$, then $s_0^{\beta_2}$ represents plain-text $\mathbf{1}$. Executing the above steps, we deduced the plain-text of $y_2$.

\paragraph{How can pseudorandom generators help?}

As we saw, if we pass pseudorandom generaturs up, this enables players to deduce the proper signals for wires. ``Testing'' a gate with the obtained signals then allows them to deduce the semantics of the signals to obtain the inputs of other parties. In this case, the gate labels are not obscured enough so that they leak some precious information.

On the other hand, if we use pseudorandom generators, we cannot deduce all the other signals, since the gate labels $A^g_{ab}$ are scrambled. This prevents us from simply reconstructing the signals (and, thus, the semantics).

\subsection{Why do we need splitters?}
\label{sec:appendix-splitters}

We illustrate the need for splitters by an example along \cite{Xu:2004:MAS:1023552}. Let us imagine the situation, where two parties hold two numbers $x_1$ and $x_2$ (each consisting of $l$ bits). The parties want to know the higher of the numbers, i.e. $\max(x_1,x_2)$.

Therefore, they could use a circuit as depicted in figure \ref{fig:max-of-numbers-xu-circuit}. It takes two $l$-bit inputs $x_1$ and $x_2$ and gives them to a comparator that outputs one bit \footnote{This output is shown as two separate wires, since it is used in several gates as input.}. This output is 1 if $x_1\geq x_2$, and 0 otherwise.

Moreover, it uses two \emph{masks}. A mask takes an $l$-bit input vector (in figure \ref{fig:max-of-numbers-xu-circuit}, depicted as the incoming wires from the bottom) and a single \emph{masking} bit (shown as a single wire from the side). Its output is a $l$-bit-vector. This vector holds zeroes only, if the masking bit is 0. Otherwise, the output vector is the same as the input vector.

\begin{figure}[t]
  \centering
  \begin{tikzpicture}
    \begin{scope}
      \draw[->,semithick] (-.25,-.50) -- (-.25,.5);
      \node at (.5,.25) {\dots};
      \draw[->,semithick] (1.25,-.50) -- (1.25,.5);
      \draw[decorate,decoration={brace,amplitude=0.5em}](1.5,-.6) -- (-.5,-.6);
      \node at (.5,-1.1) {$x_1$};
    \end{scope}
    \begin{scope}[xshift=2.5cm]
      \draw[->,semithick] (-.25,-.50) -- (-.25,.5); 
      \node at (.5,.25) {\dots}; 
      \draw[->,semithick] (1.25,-.50) -- (1.25,.5);
      \draw[decorate,decoration={brace,amplitude=0.5em}] (1.5,-.6) -- (-.5,-.6);
      \node at (.5,-1.1) {$x_2$};
    \end{scope}
    \node[shape=rectangle,minimum width=5cm,draw=black] (compi) at (1.75,.8) {Comparator};
    % x -> mask 1
    \draw[->,semithick] (-.25,-.25) -| +(-2.5,1.5);
    \node at (-2.2,1) {\dots};
    \draw[->,semithick] (1.25,0) -- ++(-1.3,0) arc(0:180:.2) -| +(-1.2,1.25);
    \node[shape=rectangle,draw=black,minimum width=2cm] (m1) at (-2.2,1.5) {Mask $M_1$};
    % y -> mask 2
    \begin{scope}[xshift=3.5cm,xscale=-1]
      \draw[->,semithick] (-.25,-.25) -| +(-2.5,1.5);
      \node at (-2.2,1) {\dots};
      \draw[->,semithick] (1.25,0) -- ++(-1.3,0) arc(0:180:.2) -| +(-1.2,1.25);+
      \node[shape=rectangle,draw=black] (neg) at (.5,1.5) {NEG};
      \node[shape=rectangle,draw=black,minimum width=2cm] (m2) at (-2.2,1.5) {Mask $M_2$};
    \end{scope}
    \draw[->,semithick] (compi.north)  +(-.1,0) |- (m1);
    \draw[->,semithick] (compi.north)  +( .1,0) |- (neg);    
    \draw[->,semithick] (neg) |- (m2);
    \node[shape=rectangle,draw=black,minimum width=3.5cm] (bo) at (1.75,3) {Bitwise OR};
    \coordinate[xshift=-.6cm] (m11) at (m1.north);
    \coordinate[xshift=.6cm] (m12) at (m1.north);
    \coordinate[xshift=-.6cm] (m21) at (m2.north);
    \coordinate[xshift=.6cm] (m22) at (m2.north);
    \draw[<-,semithick] (bo.south) +(-1.5,0) -- +(-1.5,-.5) -| (m11);
    \draw[<-,semithick] (bo.south) +(-0.5,0) -- +(-0.5,-.7) -| (m12);
    \draw[<-,semithick] (bo.south) +( .5,0) -- +( .5,-.7) -| (m21);
    \draw[<-,semithick] (bo.south) +(1.5,0) -- +(1.5,-.5) -| (m22);
    \draw[->,semithick] (bo.north) -- +(0,.4);
  \end{tikzpicture}
  \caption{A circuit representing the function $\max(x_1,x_2)$. This circuit has not the property that each wire is an used as an input wire at most once. Section \ref{sec:appendix-splitters} explains how this can be exploited by tackling the subcircuit $M_1$.}
  \label{fig:max-of-numbers-xu-circuit}
\end{figure}

Let us assume that the players already computed the outcome and saw that $x_2$ was the higher of the two numbers (i.e. player 2 had the higher number). We now consider what player 2 might do retrieve the value $x_1$ (which he was not supposed to learn).

Remember: We consider now what would happen if we would \emph{not} have used splitters, but instead just relied on the ``pure'' protocol described in \cite{Rogaway:1991:RCS:888502}.

The crucial part of this splitter-less garbled circuit is $M_1$, which is -- as a close-up -- depicted in figure \ref{fig:mask-m1}. This is a sub-circuit receiving input wires $\alpha_1,\dots,\alpha_l$ and one additional input wire $\beta$. Then each $\alpha_i$ is ANDed with $\beta$ and the results are ``returned'' as values $\gamma_i = \alpha_i \oplus \beta$. Note that in this case $\beta$ is participating in several different gates, since we did \emph{not} introduce splitters.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \draw[->,semithick] (0,0) node[below]{$\alpha_{1}$} -- (0,1);
    \draw[->,semithick] (1.5,0) node[below]{$\alpha_{2}$} -- (1.5,1);
    \node at (2.5,.5) {\dots};
    \draw[->,semithick] (4,0) node[below]{$\alpha_{l}$} -- (4,1);
    \draw[->,semithick] (-1,.25) node[left] {$\beta$} -- (-.2,.25) arc(180:0:0.2) 
    -- (1.3,.25) arc(180:0:0.2) 
    -- (3.8,.25) arc(180:0:0.2) 
    -- (4.5,.25) -- (4.5,1);
    \draw[->,semithick] (.5,.25) -- (.5,1);
    \draw[->,semithick] (2,.25) -- (2,1);
    \draw(-.25,1)rectangle(.75,1.5);
    \draw(1.25,1)rectangle(2.25,1.5);
    \draw(3.75,1)rectangle(4.75,1.5);
    \draw[->,semithick](0.25,1.5) -- +(0,.5) node[above]{$\gamma_1$};
    \draw[->,semithick](1.75,1.5) -- +(0,.5) node[above]{$\gamma_2$};
    \draw[->,semithick](4.25,1.5) -- +(0,.5) node[above]{$\gamma_l$};
    \node at (0.25,1.25) {AND};
    \node at (1.75,1.25) {AND};
    \node at (4.25,1.25) {AND};
  \end{tikzpicture}
  \caption{The mask $M_1$ from the circuit in figure \ref{fig:max-of-numbers-xu-circuit}. This sub-circuit can be exploited because one single wire -- namely wire $\beta$ -- participates in several gates. }
  \label{fig:mask-m1}
\end{figure}

Player 2 already knows that his original number $x_2$ was at least as big as $x_1$. Now, he could try what would happen if $x_1$ represented zero\footnote{That is, essentially, player 2 asks himself: ``If $x_1$ really represented $\mathbf{0}$, what could I deduce from that?''}. 

% We use a new variable $x_1'=0$ for that. In particular, player 2 sets (in his mind) every (plaintext) bit of $x_1'$ to 0. Note that he does not know the proper signals for these bits.

Since the circuit -- evaluated on $x_1$ and $x_2$ -- revealed some signals $\sigma^{\gamma_1},\dots,\sigma^{\gamma_l}$ to player 2, and we know that $x_2\geq x_1$, we know that the signals $\sigma^{\gamma_i}$ must represent semantics $\mathbf{0}$.

%But if we supply $x_1'=00\dots 0$ instead of $x_1$ the signals $\sigma^{\gamma_i}$ must \emph{still} represent semantics 0 (since $M_1$ is essentially just a collection of AND-gates). 

If all bits represented by signals $\sigma^{\alpha_i}$ represented $\mathbf{0}$, this would result in signals $\sigma^{\gamma_i}$ representing $\mathbf{0}$ even if the signal $\sigma^\beta$ represented $\mathbf{1}$ (since all the gates under consideration are AND-gates).

%This is true, even if we set $\beta$ to 1 -- we introduce again an additional variable $\beta'=1$ to denote which value we supply to the gates. That is, if we supply $\beta'$ and $x_1'$ to the sub-circuit, we still know the proper outgoing signals.

We now make a guess, that is likely wrong: We simply assume that \emph{all} signals $\sigma^{\alpha_i}$ represent plain-text $\mathbf{0}$. What follows, is essentially described by this: For the correctly-guessed bits (i.e. for those bits that are actually 0), we will get ``useful'' results, while for the wrongly-guessed bits, we will only obtain random stuff. We will now see what this means in detail.

So, let us now consider the $i$-th AND-gate within $M_1$, and look what we can deduce: 

%happens, if supply $\beta'$ and $x_1'$. Supplying $x_1'$ results in some signals $\varsigma^{\alpha_i}$. But we still know the proper output signal $\sigma^{\gamma_i}$.

We assume now that $b$ is the parity of the signal along wire $\beta$, and $a_i$ is the parity of signal on input wire $\alpha_i$. Then, for the AND-gate $g_i$, we could compute $\sigma^{\gamma_i}$ as follows:

\begin{equation}
  \label{eq:tricky-equation-for-outgoing-signals}
  \sigma^{\gamma_i} = A^i_{ab} \oplus \underbrace{G^*_b(\sigma^{\alpha_i}_{a_i})}_{G_b(\sigma^{\alpha_i}_{a_i,1}) \oplus G_b(\sigma^{\alpha_i}_{a_i,2})} \oplus \underbrace{G^*_{a_i}(\sigma^\beta_b)}_{G_{a_i}(\sigma^\beta_{b,1}) \oplus G_{a_i}(\sigma^\beta_{b,2})}
\end{equation}

Note that the term $G^*_{a_i}(\sigma^{\beta}_{b})$ participates in \emph{all} AND-gates. Moreover, note that each $a_i$ can either be 0 or 1. That means, \emph{each AND-gate} within $M_1$ has either $G^*_{0}(\sigma^{\beta}_{b})$ or $G^*_{1}(\sigma^{\beta}_{b})$ as a contribution to the signal $\sigma^{\gamma_i}$.

That is, one of these terms is present in all gates within $M_1$.

The fact we will exploit is that $G^*_{0}(\sigma^{\beta}_{b})$ and $G^*_{1}(\sigma^{\beta}_{b})$ are \emph{the same in several gates}. We can easily ``solve'' equation (\ref{eq:tricky-equation-for-outgoing-signals}) for $G^*_{a_i}(\sigma^\beta_b)$ and rename the result to $\mu_i$:

\begin{equation}
  \label{eq:mu_i-is-a-tricky-thing}
  \mu_i:=\sigma^{\gamma_i}\oplus G_b(s^{\alpha_i}_{a,1}) \oplus G_b(s^{\alpha_i}_{a,2}) \oplus A^{g_i}_{a_i b}.
\end{equation}

We consider the parts of the right-hand side:

\begin{itemize}
\item The values $\sigma^{\gamma_i}$ is known since it must represent semantics 0. Since we already evaluated the garbled circuit with $x_2\geq x_1$, we know these signals.
\item The outputs of our pseudorandom generator, $G_b(s^{\alpha_i}_{a,1})$ and $G_b(s^{\alpha_i}_{a,2})$, are known because we know the signals along wires $\alpha_i$ and can just compute the output of the pseudorandom generators $G_0$ and $G_1$ on the respective inputs.
\item The gate labels $A^{g_i}_{a_i b}$ are trivially known since they are part of the garbled circuit.
\end{itemize}

As you can see, we can compute the vales $\mu_i$ without any further complications.

Now comes a key insight. Note that $\mu_i$ contains the following information: If our guess for $x_i$ was right (i.e. if the signal $\alpha_i$ indeed represented $\mathbf{0}$), then it follows immediately that $\mu_i=G_a(s^\beta_b)$.


%If our guess for the $i$-th bit of $x_1$ was correct (i.e. if $\sigma^{\alpha_i} = \varsigma^{\alpha_i}$), then $\sigma^{\gamma_i}$ is the correct outgoing signal (for the $i$-th bit of $x_1'$). Remember that we constructed the gate labels as in equation (\ref{eqn:gate-labels-definition}). Using this definition, we can deduce that $\mu_i=G_a(s^\beta_{b1})\oplus G_a(s^\beta_{b2})$. That is, then the value $\mu_i$ depends essentially only on the signal along the wire $\beta$ (we can evaluate $G_0$ and $G_1$ without problems).

If our guess was incorrect, $\mu_i$ will be some random binary string.

This means: If we guessed the $i$-th bit of $x_1$ correct, then $\mu_i$ will be either $G_0(s^\beta_{b1})\oplus G_0(s^\beta_{b2})$ or $G_1(s^\beta_{b1})\oplus G_1(s^\beta_{b2})$. Incorrect guessed bits yield random strings.

Now, we can do the following: Note that -- in a collection of random, sufficiently long bit-strings -- the probability that a string occurs twice is rather low. But in our collection of values $\mu_i$, there might well be duplicate vales -- the ones for whom our guess of the $i$-th bit was correct. So, we collect all the values $\mu_i$ and sort them into certain ``groups'' the following way: We collect values that occur multiple times and group them. The remaining strings form the last group.

That is, in the first and second group, all the values $\mu_i$ are collected that are probably generated by $G_0(s^\beta_{b1})\oplus G_0(s^\beta_{b2})$ or $G_1(s^\beta_{b1})\oplus G_1(s^\beta_{b2})$, respectively. The third group contains all the other, random strings. All indices in the third group correspond to the bits that were wrongly guessed. Thus, these bits are probably 1 (since we guessed 0). The indices in the other groups are the correctly-guessed ones.

It can be shown that the probability that we obtain the correct bits of $x_1$ using this technique, is rather high -- too high to be acceptable in a cryptographic setting.

The weakness exploited here lay in the fact that one single value -- the signal along wire $\beta$ (resp. the output of a pseudorandom generator applied to it) -- took part in several other gates.

If we now use splitters, this ensures that not one single signal participates in several gates, but this single signal is duplicated into many other \emph{random} signals, so one cannot exploit the shown inter-gate dependencies.

Of course, this is just a very superficial treatment of this topic. For an in-depth proof of all the statements, we refer to \cite{Xu:2004:MAS:1023552}.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "seminar"
%%% End: 